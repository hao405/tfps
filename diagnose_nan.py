#!/usr/bin/env python3
"""
éç¨³æ€æ—¶åº NaN é—®é¢˜è¯Šæ–­å·¥å…·
å¿«é€Ÿæ£€æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ½œåœ¨é—®é¢˜
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os

def test_data_distribution(data_path):
    """æµ‹è¯•æ•°æ®åˆ†å¸ƒç‰¹æ€§"""
    print("\n" + "="*60)
    print("ğŸ“Š æ•°æ®åˆ†å¸ƒè¯Šæ–­")
    print("="*60)
    
    try:
        import pandas as pd
        df = pd.read_csv(data_path)
        
        # ç§»é™¤éæ•°å€¼åˆ—
        numeric_df = df.select_dtypes(include=[np.number])
        
        print(f"\næ•°æ®å½¢çŠ¶: {numeric_df.shape}")
        print(f"\nåŸºæœ¬ç»Ÿè®¡:")
        print(numeric_df.describe())
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        for col in numeric_df.columns:
            data = numeric_df[col].dropna()
            if len(data) == 0:
                continue
                
            mean = data.mean()
            std = data.std()
            min_val = data.min()
            max_val = data.max()
            
            # éç¨³æ€æ£€æµ‹
            is_constant = std < 1e-6
            has_extreme = (max_val - min_val) > 1e6
            
            if is_constant:
                print(f"\nâš ï¸  è­¦å‘Š: åˆ— '{col}' æ–¹å·®æå° (std={std:.2e})ï¼Œå¯èƒ½å¯¼è‡´RevINå¤±è´¥")
            if has_extreme:
                print(f"\nâš ï¸  è­¦å‘Š: åˆ— '{col}' èŒƒå›´æå¤§ ({min_val:.2e} to {max_val:.2e})ï¼Œå»ºè®®é¢„å¤„ç†")
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing = numeric_df.isnull().sum()
        if missing.any():
            print(f"\nâš ï¸  å‘ç°ç¼ºå¤±å€¼:")
            print(missing[missing > 0])
        
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
        return False

def test_model_initialization(args):
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("\n" + "="*60)
    print("ğŸ—ï¸  æ¨¡å‹åˆå§‹åŒ–è¯Šæ–­")
    print("="*60)
    
    try:
        from exp.exp_main import Exp_Main
        
        print(f"\né…ç½®å‚æ•°:")
        print(f"  - Model: {args.model}")
        print(f"  - d_model: {args.d_model}")
        print(f"  - T_num_expert: {args.T_num_expert}")
        print(f"  - F_num_expert: {args.F_num_expert}")
        print(f"  - beta: {args.beta}")
        print(f"  - learning_rate: {args.learning_rate}")
        
        # åˆ›å»ºå®éªŒ
        exp = Exp_Main(args)
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in exp.model.parameters())
        trainable_params = sum(p.numel() for p in exp.model.parameters() if p.requires_grad)
        
        print(f"\næ¨¡å‹å‚æ•°:")
        print(f"  - æ€»å‚æ•°: {total_params:,}")
        print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # æ£€æŸ¥DçŸ©é˜µåˆå§‹åŒ–
        if hasattr(exp.model, 'model_time'):
            D_time = exp.model.model_time.cluster.D
            D_freq = exp.model.model_frequency.cluster.D
            
            print(f"\nå­ç©ºé—´åŸºçŸ©é˜µ:")
            print(f"  - D_time shape: {D_time.shape}")
            print(f"  - D_freq shape: {D_freq.shape}")
            
            # æ£€æŸ¥æ­£äº¤æ€§
            orthogonality_time = torch.norm(D_time.t() @ D_time - torch.eye(D_time.shape[1], device=D_time.device))
            orthogonality_freq = torch.norm(D_freq.t() @ D_freq - torch.eye(D_freq.shape[1], device=D_freq.device))
            
            print(f"  - D_time æ­£äº¤æ€§è¯¯å·®: {orthogonality_time.item():.6f}")
            print(f"  - D_freq æ­£äº¤æ€§è¯¯å·®: {orthogonality_freq.item():.6f}")
            
            if orthogonality_time > 1.0 or orthogonality_freq > 1.0:
                print(f"\nâš ï¸  è­¦å‘Š: å­ç©ºé—´åŸºä¸å¤Ÿæ­£äº¤ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_forward_pass(args):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n" + "="*60)
    print("ğŸ”„ å‰å‘ä¼ æ’­è¯Šæ–­")
    print("="*60)
    
    try:
        from exp.exp_main import Exp_Main
        
        exp = Exp_Main(args)
        exp.model.eval()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 4
        seq_len = args.seq_len
        enc_in = args.enc_in
        
        # æµ‹è¯•ä¸åŒæ•°æ®èŒƒå›´
        test_cases = [
            ("æ­£å¸¸èŒƒå›´", torch.randn(batch_size, seq_len, enc_in)),
            ("å°å€¼èŒƒå›´", torch.randn(batch_size, seq_len, enc_in) * 0.01),
            ("å¤§å€¼èŒƒå›´", torch.randn(batch_size, seq_len, enc_in) * 100),
            ("å¸¸é‡è¾“å…¥", torch.ones(batch_size, seq_len, enc_in)),
        ]
        
        for name, test_input in test_cases:
            print(f"\næµ‹è¯•: {name}")
            print(f"  è¾“å…¥èŒƒå›´: [{test_input.min():.4f}, {test_input.max():.4f}]")
            
            try:
                with torch.no_grad():
                    test_input = test_input.to(exp.device)
                    
                    if 'TST' in args.model or 'Linear' in args.model:
                        s_time, s_frequency, outputs = exp.model(test_input)
                        
                        # æ£€æŸ¥è¾“å‡º
                        checks = [
                            ("s_time", s_time),
                            ("s_frequency", s_frequency),
                            ("outputs", outputs)
                        ]
                        
                        all_valid = True
                        for check_name, check_tensor in checks:
                            has_nan = torch.isnan(check_tensor).any()
                            has_inf = torch.isinf(check_tensor).any()
                            
                            if has_nan or has_inf:
                                print(f"  âŒ {check_name}: åŒ…å« NaN={has_nan}, Inf={has_inf}")
                                all_valid = False
                            else:
                                print(f"  âœ… {check_name}: OK [èŒƒå›´: {check_tensor.min():.4f}, {check_tensor.max():.4f}]")
                        
                        if not all_valid:
                            print(f"\nâš ï¸  è­¦å‘Š: {name} æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦æ£€æŸ¥æ¨¡å‹æ¶æ„")
                    else:
                        outputs = exp.model(test_input)
                        if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                            print(f"  âŒ è¾“å‡ºåŒ…å«å¼‚å¸¸å€¼")
                        else:
                            print(f"  âœ… è¾“å‡ºæ­£å¸¸")
                            
            except Exception as e:
                print(f"  âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                return False
        
        return True
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_loss_computation(args):
    """æµ‹è¯•æŸå¤±è®¡ç®—"""
    print("\n" + "="*60)
    print("ğŸ“‰ æŸå¤±è®¡ç®—è¯Šæ–­")
    print("="*60)
    
    try:
        from exp.exp_main import Exp_Main
        
        exp = Exp_Main(args)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 4
        pred_len = args.pred_len
        c_out = args.c_out
        num_expert = args.T_num_expert
        
        # æµ‹è¯•KLæ•£åº¦è®¡ç®—
        print("\næµ‹è¯•KLæ•£åº¦ç¨³å®šæ€§:")
        test_distributions = [
            ("å‡åŒ€åˆ†å¸ƒ", torch.ones(batch_size, num_expert) / num_expert),
            ("æç«¯åˆ†å¸ƒ", torch.tensor([[0.99, 0.01], [0.01, 0.99]] * (batch_size // 2)).float()),
            ("æ¥è¿‘é›¶", torch.ones(batch_size, num_expert) * 0.001),
        ]
        
        for name, pred in test_distributions[:1]:  # åªæµ‹è¯•å‡åŒ€åˆ†å¸ƒé¿å…ç»´åº¦é—®é¢˜
            if pred.shape[1] != num_expert:
                continue
                
            print(f"\n  {name}:")
            target = torch.softmax(torch.randn(batch_size, num_expert), dim=1)
            
            try:
                # æ¨¡æ‹ŸKLæ•£åº¦è®¡ç®—
                eps = 1e-8
                pred_safe = torch.clamp(pred, min=eps, max=1.0)
                target_safe = torch.clamp(target, min=eps, max=1.0)
                
                pred_safe = pred_safe / (pred_safe.sum(dim=1, keepdim=True) + eps)
                target_safe = target_safe / (target_safe.sum(dim=1, keepdim=True) + eps)
                
                log_pred = torch.log(pred_safe + eps)
                kl_loss = torch.nn.functional.kl_div(log_pred, target_safe, reduction='batchmean')
                
                if torch.isnan(kl_loss) or torch.isinf(kl_loss):
                    print(f"    âŒ KLæ•£åº¦å¼‚å¸¸: {kl_loss.item()}")
                else:
                    print(f"    âœ… KLæ•£åº¦æ­£å¸¸: {kl_loss.item():.6f}")
            except Exception as e:
                print(f"    âŒ KLæ•£åº¦è®¡ç®—å¤±è´¥: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ æŸå¤±è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»è¯Šæ–­æµç¨‹"""
    print("\n" + "="*60)
    print("ğŸ” TFPS éç¨³æ€æ—¶åº NaN é—®é¢˜è¯Šæ–­å·¥å…·")
    print("="*60)
    
    # å¯¼å…¥é…ç½®
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    try:
        from run_longExp import parser
        
        # ä½¿ç”¨æœ€å°é…ç½®è¿›è¡Œæµ‹è¯•
        test_args = [
            '--random_seed', '2021',
            '--is_training', '1',
            '--root_path', './dataset/ETT-small/',
            '--data_path', 'ETTh1.csv',
            '--model_id', 'test_diagnose',
            '--model', 'PatchTST_MoE_cluster',
            '--data', 'ETTh1',
            '--features', 'M',
            '--seq_len', '96',
            '--pred_len', '96',
            '--enc_in', '7',
            '--c_out', '7',
            '--e_layers', '1',
            '--n_heads', '4',
            '--d_model', '16',
            '--d_ff', '32',
            '--dropout', '0.1',
            '--fc_dropout', '0.1',
            '--head_dropout', '0',
            '--patch_len', '16',
            '--stride', '8',
            '--T_num_expert', '2',
            '--T_top_k', '1',
            '--F_num_expert', '2',
            '--F_top_k', '1',
            '--beta', '0.001',
            '--learning_rate', '0.001',
            '--batch_size', '8',
            '--train_epochs', '1',
            '--itr', '1'
        ]
        
        args = parser.parse_args(test_args)
        
        # è¿è¡Œè¯Šæ–­æµ‹è¯•
        results = {}
        
        # 1. æ•°æ®è¯Šæ–­
        data_path = os.path.join(args.root_path, args.data_path)
        if os.path.exists(data_path):
            results['data'] = test_data_distribution(data_path)
        else:
            print(f"\nâš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            results['data'] = False
        
        # 2. æ¨¡å‹åˆå§‹åŒ–è¯Šæ–­
        results['init'] = test_model_initialization(args)
        
        # 3. å‰å‘ä¼ æ’­è¯Šæ–­
        if results['init']:
            results['forward'] = test_forward_pass(args)
        else:
            results['forward'] = False
        
        # 4. æŸå¤±è®¡ç®—è¯Šæ–­
        if results['init']:
            results['loss'] = test_loss_computation(args)
        else:
            results['loss'] = False
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“‹ è¯Šæ–­æ€»ç»“")
        print("="*60)
        
        for test_name, result in results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"{test_name.upper()}: {status}")
        
        all_passed = all(results.values())
        
        if all_passed:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹é…ç½®æ­£å¸¸ã€‚")
            print("\nå»ºè®®:")
            print("  1. å¼€å§‹è®­ç»ƒå‰ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„betaå€¼ (0.001)")
            print("  2. ä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡ (0.0001-0.001)")
            print("  3. ç›‘æ§å‰å‡ ä¸ªepochçš„æŸå¤±å˜åŒ–")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œå»ºè®®:")
            print("  1. æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
            print("  2. é™ä½æ¨¡å‹å¤æ‚åº¦ (å‡å°‘expertæ•°é‡)")
            print("  3. ä½¿ç”¨æ›´ä¿å®ˆçš„è¶…å‚æ•°")
        
        return 0 if all_passed else 1
        
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
